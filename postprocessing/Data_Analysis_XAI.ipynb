{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XAI Experiment Data Analysis\n",
        "\n",
        "## Data Analysis todo list\n",
        "\n",
        "- ✅ Import and preprocess data\n",
        "- ✅ Demographics general overview \n",
        "- ✅ Performance (accuracy etc.)\n",
        "  - ✅ AI vs Human vs Human-AI\n",
        "  - ✅ Compare Human-AI performance among groups\n",
        "  - ✅ Compare change in performance among groups\n",
        "- ✅ Willingness to adjust judgments to match the AI system (Agreement percentage, switch percentage and AI preference)\n",
        "  - ✅ Compare agreement percentage among groups\n",
        "  - ✅ Compare switch percentage among groups\n",
        "  - ✅ Compare AI preference among groups\n",
        "- ⚙️ Effects of mistakes of the system (FP and FN)\n",
        "  - ⚙️ Performance and willingness to adjust at FP and FN compared to other news items\n",
        "  - ⚙️ Local evaluation metrics of FP and FN compared to other news items\n",
        "- [ ] Analysis of open questions\n",
        "  - [ ] What criteria do you usually use to judge whether a news/article is reliable?\n",
        "  - [ ] What other information would you like to obtain to better assess the truthfulness of an article?\n",
        "  - [ ] What functionality would be a good addition?\n",
        "\n",
        "### Bonus analyses todo list\n",
        "- [ ] Include journalists and interaction effects\n",
        "  - [ ] Compare Human-AI performance \n",
        "  - [ ] Compare change in performance\n",
        "  - [ ] Compare agreement percentage\n",
        "  - [ ] Compare switch percentage\n",
        "  - [ ] Compare AI preference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import io\n",
        "from urllib.request import urlopen\n",
        "import json\n",
        "import logging\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pandas.io.json import json_normalize\n",
        "import pingouin as pg\n",
        "import scikit_posthocs as sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import and preprocess data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read questionnaire data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/data_cleaned.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read news items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "newsitems = pd.read_csv('../preprocessing/news-items.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "newsitems[\"id\"] = newsitems.index + 1\n",
        "newsitems.index = newsitems.index + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# only keep the relevant columns\n",
        "newsitems = newsitems[[\"id\", \"label\", \"group\",\n",
        "                       \"title\", \"subtitle\", \"content\",\n",
        "                       \"source\", \"publishing_date\", \"category\",\n",
        "                       \"highlighted_content\", \"truthfulness\", \"readability\",\"natural_language_explanation\",\n",
        "                       \"is_qualification\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# only keep newsitems from group 2 and newsitems without a group (FP and TN), \n",
        "# since only these are used in the experiment\n",
        "newsitems = newsitems[(newsitems[\"group\"] == 2) | newsitems[\"group\"].isna()]\n",
        "newsitems = newsitems.drop(columns=[\"group\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a new column, \"is_fake_news\", based on if \"label\" is \"FAKE\" or \"TRUE\"\n",
        "newsitems[\"is_fake_news\"] = newsitems[\"label\"].apply(lambda x: True if x == \"FAKE\" else False)\n",
        "newsitems = newsitems.drop(columns=[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove percentage sign from truthfulness and convert to int\n",
        "newsitems[\"truthfulness\"] = newsitems[\"truthfulness\"].apply(lambda x: int(x.replace(\"%\", \"\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "newsitems.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filtering out assumed cheaters"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assume that participants who did answer at least 5 out of 6 control questions correctly were paying attention and gave valid answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# participants who did not get 2 points in the qualification shouldn't be considered\n",
        "# journalists don't have a qualification score, so they should be kept\n",
        "df = df[(df[\"POINTS.qualification\"].isna()) | (df['POINTS.qualification'] == 2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# journalists don't have a main score, so they should be kept\n",
        "df = df[(df[\"POINTS.main\"].isna()) | (df[\"POINTS.main\"] >= 5)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filter out journalists who didn't reach enough points in the merged task\n",
        "df = df[df[\"POINTS\"] >= 7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lay(data):\n",
        "    return data[data[\"JOURNALIST\"] == False]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# only lay participants are considered\n",
        "df = lay(df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demographics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get columns which start with \"demographics\"\n",
        "demographics_cols = [col for col in df.columns if col.startswith('demographics')]\n",
        "demographics_cols\n",
        "# country and nationality are not relevant, since the study was conducted in the US"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"demographics.age\"].value_counts(normalize=True).sort_index() * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"demographics.education\"].value_counts(normalize=True) * 100\n",
        "# university degree is the most common education level, this is surprising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"demographics.employment\"].value_counts(normalize=True) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "income_vc = df[\"demographics.income\"].value_counts(normalize=True)\n",
        "print(\"less-than-20000-usd\", income_vc[\"less-than-20000-usd\"] * 100)\n",
        "print(\"20000-34999-usd\", income_vc[\"20000-34999-usd\"] * 100)\n",
        "print(\"35000-49999-usd\", income_vc[\"35000-49999-usd\"] * 100)\n",
        "print(\"50000-74999-usd\", income_vc[\"50000-74999-usd\"] * 100)\n",
        "print(\"75000-99999-usd\", income_vc[\"75000-99999-usd\"] * 100)\n",
        "print(\"over-100000-usd\", income_vc[\"over-100000-usd\"] * 100)\n",
        "print(\"no-answer\", income_vc[\"no-answer\"] * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"demographics.gender\"].value_counts(normalize=True) * 100"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Kruskal-Wallis test function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def highlight(data, highlight_any=False, alpha=0.05):\n",
        "    columns = [\"p-val\", \"p-unc\", \"p-corr\", \"p\", \"pval\"]\n",
        "\n",
        "    if highlight_any:\n",
        "        columns = data.columns\n",
        "        \n",
        "    return data.style.apply(lambda x: [\"text-decoration: underline\" if isinstance(v, (int, float)) and v < alpha and c in columns else \"\" for c, v in zip(x.index, x)], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_kruskal_with_posthoc(df, col, print_results=True, between=\"FEATURE\"):\n",
        "    kruskal_test = pg.kruskal(df, dv=col, between=between, detailed=True)\n",
        "    kruskal_pval = kruskal_test[\"p-unc\"].values[0]\n",
        "\n",
        "    # uses Mann–Whitney U test under the hood\n",
        "    posthoc = pg.pairwise_tests(df, dv=col, between=between, parametric=False, padjust=\"holm\")\n",
        "\n",
        "    # posthoc = sp.posthoc_dunn(df, val_col=col, group_col=between, p_adjust=\"holm\")\n",
        "\n",
        "    if print_results:\n",
        "        print(\"Column name:\", col)\n",
        "        print(\"kruskal_pval\", kruskal_pval, \"\\n\")\n",
        "        print(posthoc, \"\\n\")\n",
        "        print(df.groupby(between)[col].describe()[[\"mean\", \"std\"]], \"\\n\")\n",
        "\n",
        "    return kruskal_test, posthoc, df.groupby(between)[col].describe()[[\"count\", \"mean\", \"std\"]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AI vs Human vs Human-AI\n",
        "\n",
        "How does the AI system perform compared to humans and the combination of humans and AI? \n",
        "\n",
        "The task is to judge whether a news item is fake news or not, thus this is a binary classification problem. Since both the AI and participants are asked to rate the news items on a 0-100 scale, we use a threshold of 50, where ratings of <50 are treated as predicted fake news and ratings >=50 are treated as predicted truthful news.\n",
        "\n",
        "The metrics for the AI are calculated over the 8 news items. Metrics are calculated for each participant separately over the presented news items (e.g. a participant correctly judged 6 out of 8 news items, thus the accuracy for the participant is 6/8 = 0.75). Human-AI performance is measured after presenting the AI rating to the participants, this is also calculated for each participant separately."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate performance metrics of the AI system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RATING_THRESHOLD = 50\n",
        "\n",
        "newsitems[\"is_fake_news_pred\"] = newsitems[\"truthfulness\"] < RATING_THRESHOLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "newsitems[\"is_fake_news\"].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the dataset is unbalanced (5 fake news items and 3 truthful news items), we also use metrics that are more robust to unbalanced datasets than accuracy, such as precision, recall, F1-score and ROC AUC. Furthermore, a Brier score is calculated to measure the accuracy of the predicted probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, brier_score_loss, log_loss\n",
        "\n",
        "ai_accuracy = accuracy_score(newsitems[\"is_fake_news\"], newsitems[\"is_fake_news_pred\"])\n",
        "ai_precision = precision_score(newsitems[\"is_fake_news\"], newsitems[\"is_fake_news_pred\"])\n",
        "ai_recall = recall_score(newsitems[\"is_fake_news\"], newsitems[\"is_fake_news_pred\"])\n",
        "ai_f1 = f1_score(newsitems[\"is_fake_news\"], newsitems[\"is_fake_news_pred\"])\n",
        "ai_roc_auc = roc_auc_score(newsitems[\"is_fake_news\"], 1 - (newsitems[\"truthfulness\"] / 100))\n",
        "ai_brier_score = brier_score_loss(\n",
        "    newsitems[\"is_fake_news\"],\n",
        "    newsitems[\"truthfulness\"] / 100, \n",
        "    pos_label=0\n",
        ")\n",
        "\n",
        "ai_performance_df = pd.DataFrame({\n",
        "  \"accuracy\": [ai_accuracy],\n",
        "  \"precision\": [ai_precision],\n",
        "  \"recall\": [ai_recall],\n",
        "  \"f1\": [ai_f1],\n",
        "  \"roc_auc_score\": [ai_roc_auc],\n",
        "  \"brier_score\": [ai_brier_score]\n",
        "})\n",
        "\n",
        "ai_performance_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate performance metrics of the participants and Human-AI teams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_human_metrics(row):\n",
        "    y_true = newsitems[\"is_fake_news\"]\n",
        "    y_pred_human = []\n",
        "    y_pred_human_prob = []\n",
        "    y_pred_human_ai = []\n",
        "    y_pred_human_ai_prob = []\n",
        "\n",
        "    for i in y_true.index:\n",
        "        y_pred_human.append(row[f\"newsitem.{i}.rating-before-xai\"] < 50)\n",
        "        y_pred_human_prob.append(row[f\"newsitem.{i}.rating-before-xai\"] / 100)\n",
        "        y_pred_human_ai.append(row[f\"newsitem.{i}.rating-after-xai\"] < 50)\n",
        "        y_pred_human_ai_prob.append(row[f\"newsitem.{i}.rating-after-xai\"] / 100)\n",
        "\n",
        "    for i, _id in enumerate(y_true.index):\n",
        "        # will be used later for comparing news items\n",
        "        row[f\"newsitem.{_id}.rating-before-correct\"] = y_true[_id] == y_pred_human[i]\n",
        "        row[f\"newsitem.{_id}.rating-after-correct\"] = y_true[_id] == y_pred_human_ai[i]\n",
        "\n",
        "    human_accuracy = accuracy_score(y_true, y_pred_human)\n",
        "    human_precision = precision_score(y_true, y_pred_human, zero_division=0)\n",
        "    human_recall = recall_score(y_true, y_pred_human)\n",
        "    human_f1 = f1_score(y_true, y_pred_human)\n",
        "    human_roc_auc = roc_auc_score(y_true, 1 - np.array(y_pred_human_prob))\n",
        "    human_brier_score = brier_score_loss(y_true, y_pred_human_prob, pos_label=0)\n",
        "\n",
        "    human_ai_accuracy = accuracy_score(y_true, y_pred_human_ai)\n",
        "    human_ai_precision = precision_score(y_true, y_pred_human_ai, zero_division=0)\n",
        "    human_ai_recall = recall_score(y_true, y_pred_human_ai)\n",
        "    human_ai_f1 = f1_score(y_true, y_pred_human_ai)\n",
        "    human_ai_roc_auc = roc_auc_score(y_true, 1 - np.array(y_pred_human_ai_prob))\n",
        "    human_ai_brier_score = brier_score_loss(y_true, y_pred_human_ai_prob, pos_label=0)\n",
        "\n",
        "    row[\"human_accuracy\"] = human_accuracy\n",
        "    row[\"human_precision\"] = human_precision\n",
        "    row[\"human_recall\"] = human_recall\n",
        "    row[\"human_f1\"] = human_f1\n",
        "    row[\"human_roc_auc_score\"] = human_roc_auc\n",
        "    row[\"human_brier_score\"] = human_brier_score\n",
        "\n",
        "    row[\"human_ai_accuracy\"] = human_ai_accuracy\n",
        "    row[\"human_ai_precision\"] = human_ai_precision\n",
        "    row[\"human_ai_recall\"] = human_ai_recall\n",
        "    row[\"human_ai_f1\"] = human_ai_f1\n",
        "    row[\"human_ai_roc_auc_score\"] = human_ai_roc_auc\n",
        "    row[\"human_ai_brier_score\"] = human_ai_brier_score\n",
        "\n",
        "    return row\n",
        "\n",
        "df = df.apply(calculate_human_metrics, axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Compare AI vs Human vs Human-AI (without journalists)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To compare the performance of the AI system to the performance of the participants and Human-AI teams, we average the metrics across the participants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show mean human scores, mean human_ai scores and ai scores in one dataframe\n",
        "human_scores = df[[\n",
        "    \"human_accuracy\", \n",
        "    \"human_precision\", \n",
        "    \"human_recall\", \n",
        "    \"human_f1\", \n",
        "    \"human_roc_auc_score\",\n",
        "    \"human_brier_score\",\n",
        "]].copy()\n",
        "\n",
        "human_ai_scores = df[[\n",
        "    \"human_ai_accuracy\", \n",
        "    \"human_ai_precision\", \n",
        "    \"human_ai_recall\", \n",
        "    \"human_ai_f1\", \n",
        "    \"human_ai_roc_auc_score\",\n",
        "    \"human_ai_brier_score\",\n",
        "]].copy()\n",
        "\n",
        "human_scores = human_scores.rename(columns=lambda x: x.replace(\"human_\", \"\"))\n",
        "human_ai_scores = human_ai_scores.rename(columns=lambda x: x.replace(\"human_ai_\", \"\"))\n",
        "\n",
        "human_scores = human_scores.mean()\n",
        "human_ai_scores = human_ai_scores.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_df = pd.DataFrame({\n",
        "  \"ai\": ai_performance_df.iloc[0],\n",
        "  \"human\": human_scores,\n",
        "  \"human_ai\": human_ai_scores,\n",
        "})\n",
        "\n",
        "scores_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_plot_df = pd.melt(\n",
        "  scores_df.drop([\"brier_score\"]).reset_index(), \n",
        "  id_vars=[\"index\"], \n",
        "  value_vars=[\"ai\", \"human\", \"human_ai\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "c = sns.catplot(x=\"variable\", y=\"value\", hue=\"index\", data=scores_plot_df,  palette=\"muted\", kind=\"bar\")\n",
        "plt.title(\"Performances of AI vs Humans vs Human-AI teams\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xlabel(\"Model\")\n",
        "c._legend.set_title(\"Metric\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Human vs Human-AI\n",
        "\n",
        "We use a Wilcoxon signed-rank test to determine if there is a significant difference in participants' performance before and after seeing the AI scores (and explanations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wilcoxon_test = pg.wilcoxon(df[\"human_accuracy\"], df[\"human_ai_accuracy\"])\n",
        "highlight(wilcoxon_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[[\"human_accuracy\", \"human_ai_accuracy\"]].describe().loc[[\"mean\", \"std\"]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A significant difference in accuracy before and after seeing the AI score (p < 0.01). The average human accuracy before is 0.730911. The average human accuracy after (= Human-AI accuracy) is 0.818350. (AI accuracy is 0.750000)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### AI vs Human-AI\n",
        "\n",
        "We use a Wilcoxon signed-rank test to determine if there is a significant difference between AI performance and Human-AI performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ai_accuracy_df = pd.DataFrame({\"accuracy\": [ai_performance_df[\"accuracy\"][0]] * len(df)})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wilcoxon_test = pg.wilcoxon(ai_accuracy_df[\"accuracy\"], df[\"human_ai_accuracy\"])\n",
        "highlight(wilcoxon_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Mean Human-AI accuracy: {df[['human_ai_accuracy']].mean().values[0]}\")\n",
        "print(f\"Mean AI accuracy: {ai_performance_df[['accuracy']].mean().values[0]}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance of AI-Human teams is significantly higher than the one of the AI system alone (p < 0.01). The average AI accuracy is 0.750000. The average Human-AI accuracy is 0.818350."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare Human-AI performance among groups (v1 vs v2 vs v3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use a Kruskal-Wallis H-test to determine if there is a significant difference in the accuracy of the Human-AI teams among the groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kruskal, pairwise, stats = perform_kruskal_with_posthoc(df, \"human_ai_accuracy\", print_results=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "highlight(kruskal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stats"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A high p-value (0.851942) indicates that there is no significant difference in performance among the groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "col = \"human_ai_accuracy\"\n",
        "\n",
        "accuracy_data = df.groupby(\"FEATURE\")[col].describe()[[\"mean\", \"std\"]].reset_index()\n",
        "accuracy_data[\"FEATURE\"] = pd.Categorical(accuracy_data[\"FEATURE\"], categories=[\"basic\", \"salient\", \"explanations\"], ordered=True)\n",
        "accuracy_data = accuracy_data.sort_values(\"FEATURE\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "sns.barplot(x=\"FEATURE\", y=\"mean\", data=accuracy_data, ax=ax, palette=\"muted\", width=.4)\n",
        "ax.set_ylabel(\"Mean Human-AI accuracy\")\n",
        "ax.set_xlabel(\"XAI System Version\")\n",
        "ax.set_ylim(0, 1)\n",
        "ax.errorbar(x=accuracy_data[\"FEATURE\"], y=accuracy_data[\"mean\"], yerr=accuracy_data[\"std\"], fmt='none', c='black', capsize=5)\n",
        "ax.set_xticklabels([\"Version 1\", \"Version 2\", \"Version 3\"])\n",
        "ax.set_axisbelow(True)\n",
        "ax.yaxis.grid(True, which='major', color='grey', alpha=.25)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare the change in performance among groups (v1 vs v2 vs v3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"accuracy_change\"] = df[\"human_ai_accuracy\"] - df[\"human_accuracy\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"accuracy_change\"].describe().loc[[\"count\", \"mean\", \"std\"]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On average, the accuracy of the participants increased by 0.087438 after seeing the AI score. We use a Kruskal-Wallis H-test to determine if there is a significant difference in the change in performance of the participants among the groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kruskal, pairwise, stats = perform_kruskal_with_posthoc(df, \"accuracy_change\", print_results=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "highlight(kruskal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stats"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is no significant difference in change in performance among the groups."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Willingness to adjust judgments to match the AI system\n",
        "\n",
        "We use three metrics to measure the willingness of the participants to adjust their judgments to match the AI system:\n",
        "- Agreement percentage: percentage of news items in which the participant’s final prediction agreed with the AI’s prediction (adapted from [1] and [2])\n",
        "- Switch percentage: percentage of news items in which the participant revised their predictions to match the model’s predictions (adapted from [1] and [2])\n",
        "- AI-preference: a number between 0 and 1 that indicates the degree to which the participant prefers the AI’s prediction over their own original prediction. This is defined for individual news item ratings of one participant. It is defined as:\n",
        "\n",
        "$$\n",
        "ai\\_preference = \n",
        "\\begin{cases}\n",
        "1 & \\text{if } rating_{before} = ai\\_rating \\text{ and } ai\\_rating = rating_{after} \\\\\n",
        "0 & \\text{if } rating_{before} = ai\\_rating \\text{ and } ai\\_rating \\neq rating_{after} \\\\\n",
        "\\min(1, \\max(0, \\frac{rating_{after} - rating_{before}}{ai\\_rating - rating_{before}})) & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "[1]: https://dl.acm.org/doi/10.1145/3290605.3300509 \n",
        "[2]: https://dl.acm.org/doi/10.1145/3351095.3372852"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate willingness to adjust metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def agrees_with_ai(row, newsitem_id):\n",
        "    ai_rating = newsitems[newsitems[\"id\"] == newsitem_id][\"truthfulness\"].values[0]\n",
        "    human_rating = row[f\"newsitem.{newsitem_id}.rating-after-xai\"]\n",
        "    return (human_rating < 50) == (ai_rating < 50)\n",
        "\n",
        "def switched_ratings(row, newsitem_id):\n",
        "    ai_rating = newsitems[newsitems[\"id\"] == newsitem_id][\"truthfulness\"].values[0]\n",
        "    human_rating_before = row[f\"newsitem.{newsitem_id}.rating-before-xai\"]\n",
        "    human_rating_after = row[f\"newsitem.{newsitem_id}.rating-after-xai\"]\n",
        "    \n",
        "    return ((human_rating_before < 50) != (human_rating_after < 50)\n",
        "            ) and ((human_rating_after < 50) == (ai_rating < 50))\n",
        "\n",
        "def calculate_ai_preference(row, newsitem_id):\n",
        "    rating_before_col = f\"newsitem.{i}.rating-before-xai\"\n",
        "    rating_after_col = f\"newsitem.{i}.rating-after-xai\"\n",
        "    ai_rating = newsitems[newsitems[\"id\"] == newsitem_id][\"truthfulness\"].values[0]\n",
        "\n",
        "    if row[rating_before_col] == ai_rating:\n",
        "        if ai_rating == row[rating_after_col]:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    else:\n",
        "        return min(1, max(0, (row[rating_after_col] - row[rating_before_col]) / (ai_rating - row[rating_before_col])))\n",
        "    \n",
        "for i in range(7, 15):\n",
        "    df[f\"newsitem.{i}.agrees-with-ai\"] = df.apply(lambda row: agrees_with_ai(row, i), axis=1)\n",
        "    df[f\"newsitem.{i}.switched-to-ai\"] = df.apply(lambda row: switched_ratings(row, i), axis=1)\n",
        "    df[f\"newsitem.{i}.ai-preference\"] = df.apply(lambda row: calculate_ai_preference(row, i), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agrees_with_ai_cols = [col for col in df.columns if col.startswith('newsitem') and col.endswith('agrees-with-ai')]\n",
        "switched_to_ai_cols = [col for col in df.columns if col.startswith('newsitem') and col.endswith('switched-to-ai')]\n",
        "ai_preference_cols = [col for col in df.columns if col.startswith('newsitem') and col.endswith('ai-preference')]\n",
        "\n",
        "# calculate average AI preference over all newsitems\n",
        "df[\"agrees-with-ai\"] = df[agrees_with_ai_cols].mean(axis=1)\n",
        "df[\"switched-to-ai\"] = df[switched_to_ai_cols].mean(axis=1)\n",
        "df[\"ai-preference\"] = df[ai_preference_cols].mean(axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use Kruskal-Wallis H-tests to determine if there is a significant difference in the willingness of the participants to adjust their judgments among the groups (based on the three metrics). When the test shows a significant difference, we use Mann-Whitney U tests with a Holm correction to determine which groups differ significantly."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agreement percentage (v1 vs v2 vs v3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kruskal, posthoc, stats = perform_kruskal_with_posthoc(df, \"agrees-with-ai\", print_results=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "highlight(kruskal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "highlight(posthoc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stats"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a significant difference in **agreement percentage** among the groups (Kruskal-Wallis H-test, p=0.005217), the posthoc tests show that the agreement percentage is significantly higher in v3 than in v1 (Mann-Whitney U test, p=0.013883) or v2 (Mann-Whitney U test, p=0.010407). There is no significant difference between the agreement percentage of groups v1 and v2 (Mann-Whitney U test, p=0.864163)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Switch percentage (v1 vs v2 vs v3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kruskal, posthoc, stats = perform_kruskal_with_posthoc(df, \"switched-to-ai\", print_results=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "highlight(kruskal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "highlight(posthoc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stats"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a significant difference in **switch percentage** among the groups (Kruskal-Wallis H-test, p=0.006382), the posthoc tests show that switch percentage is significantly higher in v3 than in v2 (Mann-Whitney U test, p=0.004753). There is no significant difference between the switch percentage of groups v1 and v2 (Mann-Whitney U test, p=0.198357) or v1 and v3 (Mann-Whitney U test, p=0.198357)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AI-preference (v1 vs v2 vs v3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kruskal, posthoc, stats = perform_kruskal_with_posthoc(df, \"ai-preference\", print_results=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "highlight(kruskal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "highlight(posthoc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stats"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a significant difference in **AI-preference** among the groups (Kruskal-Wallis H-test, p=0.00012), the posthoc tests show that AI-preference is significantly higher in v3 than in v1 (Mann-Whitney U test, p=0.011090) or v2 (Mann-Whitney U test, p=0.000113). There is no significant difference between the AI-preference of groups v1 and v2 (Mann-Whitney U test, p=0.125494)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pg.cronbach_alpha(df[[\"agrees-with-ai\", \"switched-to-ai\", \"ai-preference\"]])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the three metrics aim to measure similar concepts, we calculate Cronbach's alpha to determine if the metrics are consistent. The Cronbach's alpha is 0.818663, which indicates that the metrics are consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ai_pref_data = df.groupby(\"FEATURE\")[\"ai-preference\"].describe()[[\"mean\", \"std\"]].reset_index()\n",
        "ai_pref_data[\"FEATURE\"] = pd.Categorical(ai_pref_data[\"FEATURE\"], categories=[\"basic\", \"salient\", \"explanations\"], ordered=True)\n",
        "ai_pref_data = ai_pref_data.sort_values(\"FEATURE\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "sns.barplot(x=\"FEATURE\", y=\"mean\", data=ai_pref_data, ax=ax, palette=\"muted\", width=.4)\n",
        "ax.set_ylabel(\"Mean AI preference\")\n",
        "ax.set_xlabel(\"XAI Explanation type\")\n",
        "ax.set_ylim(0, 1)\n",
        "ax.errorbar(x=ai_pref_data[\"FEATURE\"], y=ai_pref_data[\"mean\"], yerr=ai_pref_data[\"std\"], fmt='none', c='black', capsize=5)\n",
        "ax.set_xticklabels([\"Version 1\", \"Version 2\", \"Version 3\"])\n",
        "ax.set_axisbelow(True)\n",
        "ax.yaxis.grid(True, which='major', color='grey', alpha=.25)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Effects of mistakes of the system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The experiment was designed to intentionally include mistakes of the AI system. We want to investigate if the participants are able to detect these mistakes, if they are willing to adjust their judgments to match the AI system and how they rate the AI-system when looking at the mistakes.\n",
        "\n",
        "Two types of mistakes are included in the experiment:\n",
        "- **False positive**: the AI system predicts a news item to be fake, while it is actually truthful (newsitem 14)\n",
        "- **False negative**: the AI system predicts a news item to be truthful, while it is actually fake (newsitem 13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "newsitems[\"is_mistake\"] = newsitems[\"is_fake_news\"] != newsitems[\"is_fake_news_pred\"]\n",
        "newsitems[\"is_false_positive\"] = (newsitems[\"is_fake_news\"] == False) & (newsitems[\"is_fake_news_pred\"] == True)\n",
        "newsitems[\"is_false_negative\"] = (newsitems[\"is_fake_news\"] == True) & (newsitems[\"is_fake_news_pred\"] == False)\n",
        "newsitems[\"mistake_type\"] = newsitems.apply(lambda row: \"FP\" if row[\"is_false_positive\"] else \"FN\" if row[\"is_false_negative\"] else \"TP\" if row[\"is_fake_news\"] else \"TN\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "newsitems[(newsitems[\"id\"] == 13) | (newsitems[\"id\"] == 14)][[\"title\", \"subtitle\", \"content\", \"truthfulness\", \"is_fake_news\", \"is_fake_news_pred\", \"is_mistake\", \"is_false_positive\", \"is_false_negative\", \"mistake_type\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "newsitem_eval_df = pd.DataFrame()\n",
        "\n",
        "for i in range(7, 15):\n",
        "    newsitem_cols = [\n",
        "                     f\"newsitem.{i}.system-evaluation.classified-correctly\",\n",
        "                     f\"newsitem.{i}.system-evaluation.explanations-comprehensible-and-help-assess\",\n",
        "                     f\"newsitem.{i}.system-evaluation.indications-useful\",\n",
        "                     f\"newsitem.{i}.system-evaluation.understand-what-system-does\",\n",
        "                     f\"newsitem.{i}.system-evaluation.xai-features-useful\",\n",
        "                     f\"newsitem.{i}.rating-before-correct\",\n",
        "                     f\"newsitem.{i}.rating-after-correct\",\n",
        "                     f\"newsitem.{i}.ai-preference\",\n",
        "                     f\"newsitem.{i}.switched-to-ai\",\n",
        "                     \"FEATURE\"\n",
        "                    ]\n",
        "    newsitem_eval_df = pd.concat([newsitem_eval_df, df[newsitem_cols].rename(columns={col: col.split(\".\")[-1] for col in newsitem_cols}).assign(newsitem=i)])\n",
        "\n",
        "# make \"newsitem\" the first column\n",
        "cols = newsitem_eval_df.columns.tolist()\n",
        "cols = cols[-2:] + cols[:-2]\n",
        "newsitem_eval_df = newsitem_eval_df[cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# join newsitem_eval_df with newsitems to get the \"is_mistake\" column\n",
        "newsitem_eval_df = newsitem_eval_df.merge(newsitems[[\"id\", \"is_fake_news\", \"is_mistake\", \"is_false_positive\", \"is_false_negative\", \"mistake_type\"]], left_on=\"newsitem\", right_on=\"id\").drop(columns=[\"id\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "measures:\n",
        "- on newsitem level: accuracy, change in accuracy, agreement percentage, switch percentage\n",
        "- on single rating level: ai-preference, local evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Is rating the newsitems equally difficult?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use a chi-squared test to determine if there is a significant difference in correct and incorrect ratings among the news items. We look at ratings before seeing the AI score. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# perform chi2 test\n",
        "expected, observed, stats = pg.chi2_independence(newsitem_eval_df, x='newsitem', y='rating-before-correct')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "highlight(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The low p values (<0.01) show us that the newsitems are unequally difficult to rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy on newsitems before and after seeing the AI score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate average performance over newsitems\n",
        "performance_df = newsitem_eval_df.groupby([\"newsitem\"])[[\"rating-before-correct\", \"rating-after-correct\"]].mean().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "performance_df[\"accuracy-diff\"] = performance_df[\"rating-after-correct\"] - performance_df[\"rating-before-correct\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# join newsitems with performance_df to add the rating-before-correct and rating-after-correct columns\n",
        "newsitems = newsitems.merge(performance_df, left_on=\"id\", right_on=\"newsitem\").drop(columns=[\"newsitem\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize performance\n",
        "\n",
        "newsitems = newsitems.sort_values(\"rating-before-correct\")\n",
        "\n",
        "# transform data to long format\n",
        "data = newsitems[[\"id\", \"rating-before-correct\", \"rating-after-correct\"]].melt(id_vars=[\"id\"], var_name=\"measure\", value_name=\"rating\")\n",
        "sorted_ids = data[data[\"measure\"] == \"rating-before-correct\"].sort_values(\"rating\", ascending=False)[\"id\"].unique()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "sns.barplot(x=\"id\", y=\"rating\", \n",
        "  hue=\"measure\", \n",
        "  data=data, \n",
        "  ax=ax, \n",
        "  palette=\"muted\", \n",
        "  order=sorted_ids)\n",
        "\n",
        "# rename legend labels\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "# put legend outside of plot\n",
        "ax.legend(handles=handles, labels=[\"Before XAI\", \"After XAI\"], title=\"Accuracy\", bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "\n",
        "\n",
        "ax.set_ylabel(\"Accuracy before and after XAI\")\n",
        "ax.set_xlabel(\"Newsitem\")\n",
        "\n",
        "# set the ticks belonging to ids 13 and 14 to red\n",
        "for tick in ax.get_xticklabels():\n",
        "    if int(tick.get_text()) in [13, 14]:\n",
        "        tick.set_color(\"red\")\n",
        "\n",
        "ticks = [f\"{_id}\\n{newsitems[newsitems['id'] == _id].mistake_type.values[0]}\" for _id in sorted_ids]\n",
        "ax.set_xticklabels(ticks)\n",
        "ax.set_axisbelow(True)\n",
        "ax.yaxis.grid(True, which='major', color='grey', alpha=.25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graph shows that in the case of all newsitems, the accuracy of the participants increased after seeing the AI score, except for the system mistakes where it decreased.\n",
        "\n",
        "Next we look at the change in accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "blue = \"#4C72B0\"\n",
        "red = \"#DB5F57\"\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "sns.barplot(x=\"id\", y=\"accuracy-diff\", data=newsitems, ax=ax, palette=\"muted\", width=.5, order=newsitems.sort_values(\"accuracy-diff\", ascending=False)[\"id\"])\n",
        "ax.set_ylabel(\"Accuracy difference after and before XAI\")\n",
        "ax.set_xlabel(\"Newsitem\")\n",
        "\n",
        "ticks = [f\"{item.id}\\n{item.mistake_type}\" for i, item in newsitems.sort_values(\"accuracy-diff\", ascending=False).iterrows()]\n",
        "ax.set_xticklabels(ticks)\n",
        "\n",
        "# set the colors of the bars based on their value\n",
        "for i, bar in enumerate(ax.patches):\n",
        "  if bar.get_height() > 0:\n",
        "    bar.set_color(blue)\n",
        "  else:\n",
        "    bar.set_color(red)\n",
        "\n",
        "# set the last two ticks to red\n",
        "ax.get_xticklabels()[-2].set_color(\"red\")\n",
        "ax.get_xticklabels()[-1].set_color(\"red\")\n",
        "\n",
        "ax.set_axisbelow(True)\n",
        "ax.yaxis.grid(True, which='major', color='grey', alpha=.25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "newsitems.groupby(\"is_mistake\")[\"accuracy-diff\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mwu_test = pg.mwu(\n",
        "    newsitems[newsitems[\"is_mistake\"] == True][\"accuracy-diff\"],\n",
        "    newsitems[newsitems[\"is_mistake\"] == False][\"accuracy-diff\"]\n",
        ")\n",
        "\n",
        "highlight(mwu_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The MWU test shows that there is no significant difference in the change in accuracy between the newsitems (p=0.071429). **This test might not be relevant because of the low number of items (2 mistakes, 6 no mistakes)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We perform a chi-squared test to determine if there is a significant difference in the number of correct and incorrect ratings among the news items after seeing the AI score among newsitems where the AI made a mistake vs newsitems where the AI did not make a mistake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# perform chi2 test\n",
        "expected, observed, stats = pg.chi2_independence(newsitem_eval_df, x='is_mistake', y='rating-after-correct')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "highlight(stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "newsitem_eval_df.groupby(\"is_mistake\")[\"rating-after-correct\"].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define MWU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_mwu(df, dv, between):\n",
        "    # get both possible values of the between variable\n",
        "    values = df[between].unique()\n",
        "\n",
        "    if len(values) != 2:\n",
        "        raise ValueError(\"The between variable should have exactly two possible values\")\n",
        "    \n",
        "    # check if dv is a numeric variable\n",
        "    value = df[dv].values[0]\n",
        "    is_numeric = isinstance(value, (int, float, complex))\n",
        "    \n",
        "    if not is_numeric:\n",
        "        raise ValueError(\"The dependent variable should be numeric\")\n",
        "\n",
        "    mwu_test = pg.mwu(\n",
        "        df[df[between] == values[0]][dv],\n",
        "        df[df[between] == values[1]][dv]\n",
        "    )\n",
        "\n",
        "    stats = df.groupby(between)[dv].describe()[[\"count\", \"mean\", \"std\"]]\n",
        "\n",
        "    return mwu_test, stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AI-preference on newsitems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ai_preference_means = newsitem_eval_df.groupby(\"newsitem\")[\"ai-preference\"].mean().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted_ids = ai_preference_means.sort_values(\"ai-preference\", ascending=False)[\"newsitem\"].unique()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "sns.barplot(\n",
        "  x=\"newsitem\", \n",
        "  y=\"ai-preference\", \n",
        "  data=ai_preference_means, \n",
        "  ax=ax, \n",
        "  palette=\"muted\", \n",
        "  width=.5, \n",
        "  order=sorted_ids\n",
        ")\n",
        "\n",
        "ticks = [f\"{_id}\\n{newsitems[newsitems['id'] == _id].mistake_type.values[0]}\" for _id in sorted_ids]\n",
        "ax.set_xticklabels(ticks)\n",
        "\n",
        "ax.set_ylabel(\"AI-preference\")\n",
        "ax.set_xlabel(\"Newsitem\")\n",
        "\n",
        "# set the colors of the bars based on their value\n",
        "for i, bar in enumerate(ax.patches):\n",
        "  if bar.get_height() > 0:\n",
        "    bar.set_color(blue)\n",
        "  else:\n",
        "    bar.set_color(red)\n",
        "\n",
        "# set the last two ticks to red\n",
        "ax.get_xticklabels()[-2].set_color(\"red\")\n",
        "ax.get_xticklabels()[-1].set_color(\"red\")\n",
        "\n",
        "ax.set_axisbelow(True)\n",
        "ax.yaxis.grid(True, which='major', color='grey', alpha=.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mwu, stats = perform_mwu(newsitem_eval_df, dv=\"ai-preference\", between=\"is_mistake\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "highlight(mwu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Local evaluation of newsitems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "local_eval_cols = [\n",
        "    'classified-correctly', # The AI-System classified the news items correctly\n",
        "    'explanations-comprehensible-and-help-assess', # The presented explanations are comprehensible and help me with assessing the news articles\n",
        "    'indications-useful', # The indications given by the AI-System are useful to assess the truthfulness of the news article\n",
        "    'understand-what-system-does', # I understand what the AI-System does\n",
        "    'xai-features-useful' # The explainability features presented are useful to assess the truthfulness of the news article\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "local_eval_means = newsitem_eval_df.groupby('newsitem')[local_eval_cols].mean().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chart_data = local_eval_means.melt(id_vars=[\"newsitem\"], var_name=\"measure\", value_name=\"rating\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted_ids = chart_data[chart_data[\"measure\"] == \"classified-correctly\"].sort_values(\"rating\", ascending=False)[\"newsitem\"].unique()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.barplot(\n",
        "  x=\"newsitem\", \n",
        "  y=\"rating\", \n",
        "  hue=\"measure\", \n",
        "  data=chart_data, \n",
        "  ax=ax, \n",
        "  palette=\"muted\",\n",
        "  order=sorted_ids\n",
        "  )\n",
        "\n",
        "# put legend outside of the plot\n",
        "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "\n",
        "ax.set_ylabel(\"Mean local evaluation metrics\")\n",
        "ax.set_xlabel(\"Newsitem\")\n",
        "\n",
        "# set y scale to 0-7\n",
        "ax.set_ylim(1, 7)\n",
        "\n",
        "# set the ticks belonging to ids 13 and 14 to red\n",
        "for tick in ax.get_xticklabels():\n",
        "    if int(tick.get_text()) in [13, 14]:\n",
        "        tick.set_color(\"red\")\n",
        "\n",
        "ticks = [f\"{_id}\\n{newsitems[newsitems['id'] == _id].mistake_type.values[0]}\" for _id in sorted_ids]\n",
        "ax.set_xticklabels(ticks)\n",
        "\n",
        "ax.set_axisbelow(True)\n",
        "ax.yaxis.grid(True, which='major', color='grey', alpha=.25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_data = []\n",
        "\n",
        "for col in local_eval_cols:\n",
        "    mwu, stats = perform_mwu(newsitem_eval_df, dv=col, between=\"is_mistake\")\n",
        "    summary_data.append({\n",
        "        \"measure\": col,\n",
        "        \"mwu\": mwu[\"U-val\"].values[0],\n",
        "        \"mwu-p\": mwu[\"p-val\"].values[0],\n",
        "        \"mean-no-mistake\": stats.loc[False, \"mean\"],\n",
        "        \"std-no-mistake\": stats.loc[False, \"std\"],\n",
        "        \"mean-mistake\": stats.loc[True, \"mean\"],\n",
        "        \"std-mistake\": stats.loc[True, \"std\"],\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "highlight(summary_df, highlight_any=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of open questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
